{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from dataset import SNPmarkersDataset\n",
    "import json\n",
    "from utils import train_DL_model\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from Models.GPTransformer import GPTransformer, EmbeddingType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "DROPOUT = 0\n",
    "N_EMBEDDING = 3\n",
    "N_HEADS = 3\n",
    "N_LAYERS = 2\n",
    "HIDDEN_NODES = 256\n",
    "N_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_phenotypes = \"ep_res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of selected features: 3688\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mi = np.zeros(36304)\n",
    "modes = [\"local_train\", \"validation\", \"test\"]\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "for mode in modes:\n",
    "    dataset = SNPmarkersDataset(mode = mode, skip_check=True)\n",
    "    dataset.set_phenotypes = selected_phenotypes\n",
    "    \n",
    "    X = dataset.get_all_SNP()\n",
    "    y = dataset.phenotypes[selected_phenotypes]\n",
    "    \n",
    "    # Save the results to avoid fetching two times the sames values later on\n",
    "    if mode == \"local_train\":\n",
    "        X_train = X\n",
    "        y_train = y \n",
    "    if mode == \"validation\":\n",
    "        X_val = X\n",
    "        y_val = y \n",
    "\"\"\"    \n",
    "    mi += mutual_info_regression(X,y, n_jobs=-1, discrete_features=True, random_state=2307)\n",
    "\n",
    "# Divide the number of modes to obtain the average mutual information\n",
    "mi /= len(modes)\n",
    "indexes = np.where(mi < 0.02)[0]\n",
    "print(f\"Nb of selected features: {len(indexes)}\")\n",
    "\"\"\"\n",
    "mi = np.random.choice([0, 1], size=36304, p=[.9, .1])\n",
    "indexes = np.where(mi == 1)[0]\n",
    "print(f\"Nb of selected features: {len(indexes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNPResidualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "def convert_categorical_to_frequency(data, path = \"gptranformer_embedding_data.json\"):\n",
    "    with open(path,\"r\") as f:\n",
    "        freq_data = json.load(f)\n",
    "    \n",
    "    results = []\n",
    "    for sample in data:\n",
    "        func = lambda t: [freq_data[str(t[0])][\"p\"]**2, 2*freq_data[str(t[0])][\"p\"]*freq_data[str(t[0])][\"q\"],freq_data[str(t[0])][\"q\"]**2].__getitem__(t[1])\n",
    "        results.append(list(map(func, enumerate(sample))))\n",
    "    return np.array(results, dtype=np.float32)\n",
    "\n",
    "\n",
    "train_dataset = SNPResidualDataset(X_train[indexes].to_numpy(dtype=np.int32), y_train.to_numpy(dtype=np.float32))\n",
    "validation_dataset = SNPResidualDataset(X_val[indexes].to_numpy(dtype=np.int32), y_val.to_numpy(dtype=np.float32))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTransformer(\n",
    "    n_features = len(indexes),\n",
    "    embedding_size=3, \n",
    "    n_hidden=HIDDEN_NODES,\n",
    "    n_heads=N_HEADS,\n",
    "    n_blocks=N_LAYERS,\n",
    "    embedding_type = EmbeddingType.EmbeddingTable,\n",
    "    embedding_table_weight=torch.eye(3),\n",
    ")\n",
    "\n",
    "# Define function and seed to fix the loading via the dataloader (from https://pytorch.org/docs/stable/notes/randomness.html#pytorch)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = 4, worker_init_fn=seed_worker)\n",
    "validation_dataloader = data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers = 4, worker_init_fn=seed_worker)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cpu\n",
      "Model architecture : \n",
      " GPTransformer(\n",
      "  (embedding): Embedding(3, 3)\n",
      "  (preprocessing): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (transformer): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (multihead): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=3, out_features=256, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (fc2): Linear(in_features=256, out_features=3, bias=True)\n",
      "      (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (multihead): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=3, out_features=256, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (fc2): Linear(in_features=256, out_features=3, bias=True)\n",
      "      (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output): Linear(in_features=11064, out_features=1, bias=True)\n",
      ")\n",
      "Numbers of parameters: 14784\n",
      "Optimizer used: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Train feature batch shape: torch.Size([8, 3688])\n",
      "Train labels batch shape: torch.Size([8])\n",
      "Validation feature batch shape: torch.Size([8, 3688])\n",
      "Validation labels batch shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "train_DL_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    N_EPOCHS,\n",
    "    criterion,\n",
    "    phenotype=selected_phenotypes,\n",
    "    log_wandb=False,\n",
    "    early_stop_n_epoch=5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
