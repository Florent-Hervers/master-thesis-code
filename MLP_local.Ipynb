{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as data\n",
    "from dataset import SNPmarkersDataset\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from utils import format_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, nlayers: int = 1, hidden_nodes: list[int] = [], dropout: float = 0):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        if dropout < 0 or dropout >= 1:\n",
    "            raise AttributeError(\"The dropout must be between 0 and 1\")\n",
    "\n",
    "        if nlayers < 1:\n",
    "            raise AttributeError(\"The number of layers must be greater or equal than one !\")\n",
    "        \n",
    "        if len(hidden_nodes) != nlayers - 1:\n",
    "            raise AttributeError(f\"Not enough hidden_nodes given, expected a list of length {nlayers - 1} but got one of {len(hidden_nodes)}\")\n",
    "\n",
    "        # Use a copy to avoid modifying the hyperparameter value for future runs\n",
    "        hidden_nodes_model = hidden_nodes.copy()\n",
    "        hidden_nodes_model.insert(0, 36304)\n",
    "        hidden_nodes_model.append(1)\n",
    "\n",
    "        self.model = nn.Sequential(*[LinearBlock(hidden_nodes_model[i], hidden_nodes_model[i + 1], dropout=dropout) for i in range(nlayers - 1)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(hidden_nodes_model[-2], hidden_nodes_model[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output_layer(self.dropout(self.model(x)))\n",
    "\n",
    "class LinearBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout = 0):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(in_features=input_size, out_features=output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return F.relu(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT = 0.25\n",
    "N_LAYERS = 2\n",
    "HIDDEN_NODES = [1024]\n",
    "N_EPOCHS = 5\n",
    "SCHEDULER_STEP_SIZE = 20\n",
    "SCHEDULER_REDUCE_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SNPmarkersDataset(mode = \"local_train\", skip_check=True)\n",
    "validation_dataset = SNPmarkersDataset(mode = \"validation\", skip_check=True)\n",
    "selected_phenotypes = [\"size_res\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture : \n",
      " MLP(\n",
      "  (model): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "      (fc): Linear(in_features=36304, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (output_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "Numbers of parameters: 37177345\n",
      "Finished training for epoch 0. Train loss: 7.468574290095648.\n",
      "Validation step for epoch 0 finished! Validation loss: 0.7992635311043197. Correlation: 0.06521497039918506\n",
      "Finished training for epoch 1. Train loss: 0.8144107741652897.\n",
      "Validation step for epoch 1 finished! Validation loss: 0.7995851254425859. Correlation: 0.14155396351053293\n",
      "Finished training for epoch 2. Train loss: 0.7953937512403109.\n",
      "Validation step for epoch 2 finished! Validation loss: 0.807317291283051. Correlation: 0.18208576635510643\n",
      "Finished training for epoch 3. Train loss: 0.8382322407706999.\n",
      "Validation step for epoch 3 finished! Validation loss: 0.8378236105112005. Correlation: 0.001580592527463037\n",
      "Finished training for epoch 4. Train loss: 0.8373185137842271.\n",
      "Validation step for epoch 4 finished! Validation loss: 0.8022268699041556. Correlation: 0.13075204906058435\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for phenotype in selected_phenotypes:\n",
    "    train_dataset.set_phenotypes = phenotype\n",
    "    train_dataloader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = 4)\n",
    "    \n",
    "    validation_dataset.set_phenotypes = phenotype\n",
    "    validation_dataloader = data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers = 4)\n",
    "\n",
    "    model = MLP(nlayers=N_LAYERS, hidden_nodes= HIDDEN_NODES, dropout= DROPOUT)\n",
    "    print(f\"Model architecture : \\n {model}\")\n",
    "    print(f\"Numbers of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = SCHEDULER_STEP_SIZE, gamma = SCHEDULER_REDUCE_RATIO)\n",
    "    criteron = torch.nn.L1Loss()\n",
    "    model.to(device)\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for x,y in train_dataloader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            y = y.view(-1,1)\n",
    "            loss = criteron(output, y)\n",
    "            train_loss.append(loss.cpu().detach())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Finished training for epoch {epoch}. Train loss: {np.array(train_loss).mean()}.\")\n",
    "\n",
    "        val_loss = []\n",
    "        predicted = []\n",
    "        target = []\n",
    "        model.eval()\n",
    "        for x,y in validation_dataloader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            y = y.view(-1,1)\n",
    "            loss = criteron(output, y)\n",
    "            val_loss.append(loss.cpu().detach())\n",
    "            if len(predicted) == 0:\n",
    "                predicted = output.cpu().detach()\n",
    "                target = y.cpu().detach()\n",
    "            else:\n",
    "                predicted = np.concatenate((predicted, output.cpu().detach()), axis = 0)\n",
    "                target = np.concatenate((target, y.cpu().detach()), axis = 0)\n",
    "        \n",
    "        predicted = predicted.reshape((predicted.shape[0],))\n",
    "        \n",
    "        target = target.reshape((target.shape[0],))\n",
    "        scheduler.step()\n",
    "        print(f\"Validation step for epoch {epoch} finished! Validation loss: {np.array(val_loss).mean()}. Correlation: {pearsonr(predicted, target).statistic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
