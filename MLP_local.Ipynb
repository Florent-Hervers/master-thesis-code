{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as data\n",
    "from dataset import SNPmarkersDataset\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from utils import format_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, nlayers: int = 1, hidden_nodes: list[int] = [], dropout: float = 0):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        if dropout < 0 or dropout >= 1:\n",
    "            raise AttributeError(\"The dropout must be between 0 and 1\")\n",
    "\n",
    "        if nlayers < 1:\n",
    "            raise AttributeError(\"The number of layers must be greater or equal than one !\")\n",
    "        \n",
    "        if len(hidden_nodes) != nlayers - 1:\n",
    "            raise AttributeError(f\"Not enough hidden_nodes given, expected a list of length {nlayers - 1} but got one of {len(hidden_nodes)}\")\n",
    "\n",
    "        # Use a copy to avoid modifying the hyperparameter value for future runs\n",
    "        hidden_nodes_model = hidden_nodes.copy()\n",
    "        hidden_nodes_model.insert(0, 36304)\n",
    "        hidden_nodes_model.append(1)\n",
    "\n",
    "        self.model = nn.Sequential(*[LinearBlock(hidden_nodes_model[i], hidden_nodes_model[i + 1], dropout=dropout) for i in range(nlayers - 1)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(hidden_nodes_model[-2], hidden_nodes_model[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output_layer(self.dropout(self.model(x)))\n",
    "\n",
    "class LinearBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout = 0):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(in_features=input_size, out_features=output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return F.relu(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT = 0.25\n",
    "N_LAYERS = 2\n",
    "HIDDEN_NODES = [1024]\n",
    "N_EPOCHS = 45\n",
    "SCHEDULER_STEP_SIZE = 20\n",
    "SCHEDULER_REDUCE_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SNPmarkersDataset(mode = \"local_train\", skip_check=True)\n",
    "validation_dataset = SNPmarkersDataset(mode = \"validation\", skip_check=True)\n",
    "selected_phenotypes = [\"size_res\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture : \n",
      " MLP(\n",
      "  (model): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "      (fc): Linear(in_features=36304, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (output_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "Numbers of parameters: 37177345\n",
      "Finished training for epoch 0. Train loss: 12.412681449401774.\n",
      "Validation step for epoch 0 finished! Validation loss: 6.129100133557539. Correlation: 0.05412116201192632\n",
      "Finished training for epoch 1. Train loss: 6.256941585948063.\n",
      "Validation step for epoch 1 finished! Validation loss: 6.057724309456244. Correlation: 0.10148030344103688\n",
      "Finished training for epoch 2. Train loss: 6.267938825846788.\n",
      "Validation step for epoch 2 finished! Validation loss: 5.862513470736242. Correlation: 0.20827596528752554\n",
      "Finished training for epoch 3. Train loss: 5.942596909191721.\n",
      "Validation step for epoch 3 finished! Validation loss: 5.397790124357036. Correlation: 0.41055046250247235\n",
      "Finished training for epoch 4. Train loss: 5.709637889370155.\n",
      "Validation step for epoch 4 finished! Validation loss: 5.0887440075648245. Correlation: 0.5083028048171561\n",
      "Finished training for epoch 5. Train loss: 5.786096274222539.\n",
      "Validation step for epoch 5 finished! Validation loss: 5.263756311204514. Correlation: 0.4394595623151024\n",
      "Finished training for epoch 6. Train loss: 5.350590996208675.\n",
      "Validation step for epoch 6 finished! Validation loss: 4.701285098415994. Correlation: 0.5926637430561482\n",
      "Finished training for epoch 7. Train loss: 5.425007003815672.\n",
      "Validation step for epoch 7 finished! Validation loss: 4.62204895970321. Correlation: 0.5965909692150556\n",
      "Finished training for epoch 8. Train loss: 5.124514365429187.\n",
      "Validation step for epoch 8 finished! Validation loss: 4.320721988829933. Correlation: 0.6532494377527189\n",
      "Finished training for epoch 9. Train loss: 4.776413962860092.\n",
      "Validation step for epoch 9 finished! Validation loss: 4.139161194814323. Correlation: 0.7001070009173804\n",
      "Finished training for epoch 10. Train loss: 4.885348959378866.\n",
      "Validation step for epoch 10 finished! Validation loss: 4.036642866882217. Correlation: 0.714731752209634\n",
      "Finished training for epoch 11. Train loss: 5.211081815257544.\n",
      "Validation step for epoch 11 finished! Validation loss: 3.9852944722115464. Correlation: 0.7158012261192255\n",
      "Finished training for epoch 12. Train loss: 4.896255753875981.\n",
      "Validation step for epoch 12 finished! Validation loss: 3.913481690995063. Correlation: 0.7345755889294647\n",
      "Finished training for epoch 13. Train loss: 4.959541660273728.\n",
      "Validation step for epoch 13 finished! Validation loss: 4.201638666991139. Correlation: 0.6831909954175752\n",
      "Finished training for epoch 14. Train loss: 4.810598870692573.\n",
      "Validation step for epoch 14 finished! Validation loss: 3.788237924300084. Correlation: 0.7505048164431745\n",
      "Finished training for epoch 15. Train loss: 4.451994565290262.\n",
      "Validation step for epoch 15 finished! Validation loss: 3.6390484872463005. Correlation: 0.7666480145636954\n",
      "Finished training for epoch 16. Train loss: 4.347443531900749.\n",
      "Validation step for epoch 16 finished! Validation loss: 3.5957530486469365. Correlation: 0.7680149313642843\n",
      "Finished training for epoch 17. Train loss: 4.609520059069014.\n",
      "Validation step for epoch 17 finished! Validation loss: 3.535189165532005. Correlation: 0.7811294741552846\n",
      "Finished training for epoch 18. Train loss: 4.066750144228475.\n",
      "Validation step for epoch 18 finished! Validation loss: 3.4515319376066. Correlation: 0.7885477168124342\n",
      "Finished training for epoch 19. Train loss: 4.245535256135504.\n",
      "Validation step for epoch 19 finished! Validation loss: 3.611090913193405. Correlation: 0.7668577065424712\n",
      "Finished training for epoch 20. Train loss: 4.188784133686887.\n",
      "Validation step for epoch 20 finished! Validation loss: 3.75610068118585. Correlation: 0.7582647741478039\n",
      "Finished training for epoch 21. Train loss: 4.0110812868199615.\n",
      "Validation step for epoch 21 finished! Validation loss: 3.4783496725233762. Correlation: 0.7876142606901805\n",
      "Finished training for epoch 22. Train loss: 4.068320937192629.\n",
      "Validation step for epoch 22 finished! Validation loss: 3.80771437724399. Correlation: 0.7437504229653712\n",
      "Finished training for epoch 23. Train loss: 4.070529897449574.\n",
      "Validation step for epoch 23 finished! Validation loss: 3.7424384489366664. Correlation: 0.7503634724403005\n",
      "Finished training for epoch 24. Train loss: 4.229053132151315.\n",
      "Validation step for epoch 24 finished! Validation loss: 3.4896415182318137. Correlation: 0.78131623807323\n",
      "Finished training for epoch 25. Train loss: 3.7804988208285786.\n",
      "Validation step for epoch 25 finished! Validation loss: 3.446614678467386. Correlation: 0.7821299966173394\n",
      "Finished training for epoch 26. Train loss: 3.9735842578226572.\n",
      "Validation step for epoch 26 finished! Validation loss: 3.3766812261412222. Correlation: 0.7926546737372259\n",
      "Finished training for epoch 27. Train loss: 3.6476282685009345.\n",
      "Validation step for epoch 27 finished! Validation loss: 3.4150045019793587. Correlation: 0.7946144296130857\n",
      "Finished training for epoch 28. Train loss: 4.046430636604042.\n",
      "Validation step for epoch 28 finished! Validation loss: 3.1860336099315454. Correlation: 0.81967414548778\n",
      "Finished training for epoch 29. Train loss: 3.887893979645605.\n",
      "Validation step for epoch 29 finished! Validation loss: 3.0209950166920105. Correlation: 0.8393709442303953\n",
      "Finished training for epoch 30. Train loss: 3.8661661649578365.\n",
      "Validation step for epoch 30 finished! Validation loss: 2.9642696739136425. Correlation: 0.8405481519452673\n",
      "Finished training for epoch 31. Train loss: 3.725049359651838.\n",
      "Validation step for epoch 31 finished! Validation loss: 3.032218998287992. Correlation: 0.8365760694329754\n",
      "Finished training for epoch 32. Train loss: 3.889320963458021.\n",
      "Validation step for epoch 32 finished! Validation loss: 3.1943358934713593. Correlation: 0.8232488607186397\n",
      "Finished training for epoch 33. Train loss: 3.6739938850891.\n",
      "Validation step for epoch 33 finished! Validation loss: 3.181693892150097. Correlation: 0.8247465468567287\n",
      "Finished training for epoch 34. Train loss: 3.7871253936831555.\n",
      "Validation step for epoch 34 finished! Validation loss: 3.2747736238299754. Correlation: 0.8081461023301034\n",
      "Finished training for epoch 35. Train loss: 3.6745193596361845.\n",
      "Validation step for epoch 35 finished! Validation loss: 3.305886276583622. Correlation: 0.8115153498825333\n",
      "Finished training for epoch 36. Train loss: 3.7184212323079797.\n",
      "Validation step for epoch 36 finished! Validation loss: 3.5463354744814533. Correlation: 0.7880399635066168\n",
      "Finished training for epoch 37. Train loss: 3.66271985961462.\n",
      "Validation step for epoch 37 finished! Validation loss: 2.9941091157733952. Correlation: 0.838692273374956\n",
      "Finished training for epoch 38. Train loss: 3.6796805450248873.\n",
      "Validation step for epoch 38 finished! Validation loss: 2.956801552380455. Correlation: 0.8402158238393972\n",
      "Finished training for epoch 39. Train loss: 3.6819756734815785.\n",
      "Validation step for epoch 39 finished! Validation loss: 3.460100465195065. Correlation: 0.791111653444042\n",
      "Finished training for epoch 40. Train loss: 3.6178775350491525.\n",
      "Validation step for epoch 40 finished! Validation loss: 3.476895143765419. Correlation: 0.7990261150000266\n",
      "Finished training for epoch 41. Train loss: 3.431516776675461.\n",
      "Validation step for epoch 41 finished! Validation loss: 3.4561148553450467. Correlation: 0.7989320285152093\n",
      "Finished training for epoch 42. Train loss: 3.356331692424908.\n",
      "Validation step for epoch 42 finished! Validation loss: 3.363514116856434. Correlation: 0.8105588152481236\n",
      "Finished training for epoch 43. Train loss: 3.5940586651470796.\n",
      "Validation step for epoch 43 finished! Validation loss: 3.313480780953278. Correlation: 0.8118577822985836\n",
      "Finished training for epoch 44. Train loss: 3.496797457259868.\n",
      "Validation step for epoch 44 finished! Validation loss: 3.2831360623509105. Correlation: 0.817756986078881\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for phenotype in selected_phenotypes:\n",
    "    train_dataset.set_phenotypes = phenotype\n",
    "    train_dataloader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = 4)\n",
    "    \n",
    "    validation_dataset.set_phenotypes = phenotype\n",
    "    validation_dataloader = data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers = 4)\n",
    "    \n",
    "\n",
    "    model = MLP(nlayers=N_LAYERS, hidden_nodes= HIDDEN_NODES, dropout= DROPOUT)\n",
    "    print(f\"Model architecture : \\n {model}\")\n",
    "    print(f\"Numbers of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = SCHEDULER_STEP_SIZE, gamma = SCHEDULER_REDUCE_RATIO)\n",
    "    criteron = torch.nn.L1Loss()\n",
    "    model.to(device)\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for x,y in train_dataloader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            y = y.view(-1,1)\n",
    "            loss = criteron(output, y)\n",
    "            train_loss.append(loss.cpu().detach())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Finished training for epoch {epoch}. Train loss: {np.array(train_loss).mean()}.\")\n",
    "\n",
    "        val_loss = []\n",
    "        predicted = []\n",
    "        target = []\n",
    "        model.eval()\n",
    "        for x,y in validation_dataloader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            y = y.view(-1,1)\n",
    "            loss = criteron(output, y)\n",
    "            val_loss.append(loss.cpu().detach())\n",
    "            if len(predicted) == 0:\n",
    "                predicted = output.cpu().detach()\n",
    "                target = y.cpu().detach()\n",
    "            else:\n",
    "                predicted = np.concatenate((predicted, output.cpu().detach()), axis = 0)\n",
    "                target = np.concatenate((target, y.cpu().detach()), axis = 0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        predicted = predicted.reshape((predicted.shape[0],))\n",
    "        target = target.reshape((target.shape[0],))\n",
    "        scheduler.step()\n",
    "        print(f\"Validation step for epoch {epoch} finished! Validation loss: {np.array(val_loss).mean()}. Correlation: {pearsonr(predicted, target).statistic}\")\n",
    "    if phenotype == \"de_res\":\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
